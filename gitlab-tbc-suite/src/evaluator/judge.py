"""
Phase 2: LLM Judge - Compare outputs and generate scores.

Uses Claude as a judge to compare actual output against expected output
and generate scores with detailed analysis.
"""

import json
import re
from typing import Any

from claude_agent_sdk import query, ClaudeAgentOptions
from claude_agent_sdk.types import AssistantMessage, TextBlock, ResultMessage

from .models import EvaluationScore, JudgeAnalysis


JUDGE_SYSTEM_PROMPT = """You are an expert evaluator for GitLab CI/CD configurations using the To-Be-Continuous (TBC) framework.

Your role is to compare a generated configuration against an expected output and provide:
1. Numerical scores (0-100) for each dimension
2. Lists of correct, incorrect, missing, and extra elements
3. Clear reasoning for your assessment
4. Actionable suggestions for improvement

## Scoring Dimensions

- **structural**: Is the YAML valid and properly structured? Are stages, jobs, and includes organized correctly?
- **semantic**: Does it use the correct TBC templates, inputs, and variables? Is the CI/CD logic correct?
- **completeness**: Are all required components present (templates, variables, secrets, environments)?
- **correctness**: Are the values and configurations accurate for the requirements?
- **overall**: Weighted average considering all dimensions

## Scoring Guidelines

- 90-100: Excellent - Minor or no differences, functionally equivalent
- 70-89: Good - Some differences but core functionality correct
- 50-69: Acceptable - Missing elements or errors but partially correct
- 30-49: Poor - Significant issues affecting functionality
- 0-29: Failed - Major structural or functional problems

## Evaluation Criteria

Be fair but thorough:
- Minor formatting differences (spacing, ordering) should NOT significantly impact scores
- Focus on functional equivalence and best practices
- Consider if the output would work correctly in a real GitLab pipeline
- TBC-specific: Check template versions, input variables, and component mode usage

IMPORTANT: Respond ONLY with valid JSON matching the exact schema provided. No additional text."""


EVALUATION_SCHEMA = {
    "type": "object",
    "properties": {
        "scores": {
            "type": "object",
            "properties": {
                "overall": {"type": "integer", "minimum": 0, "maximum": 100},
                "structural": {"type": "integer", "minimum": 0, "maximum": 100},
                "semantic": {"type": "integer", "minimum": 0, "maximum": 100},
                "completeness": {"type": "integer", "minimum": 0, "maximum": 100},
                "correctness": {"type": "integer", "minimum": 0, "maximum": 100}
            },
            "required": ["overall", "structural", "semantic", "completeness", "correctness"]
        },
        "correct_elements": {
            "type": "array",
            "items": {"type": "string"},
            "description": "List of elements that are correct in the actual output"
        },
        "incorrect_elements": {
            "type": "array",
            "items": {"type": "string"},
            "description": "List of elements that are incorrect in the actual output"
        },
        "missing_elements": {
            "type": "array",
            "items": {"type": "string"},
            "description": "List of elements missing from the actual output"
        },
        "extra_elements": {
            "type": "array",
            "items": {"type": "string"},
            "description": "List of extra elements in the actual output not in expected"
        },
        "reasoning": {
            "type": "string",
            "description": "Detailed reasoning for the scores"
        },
        "suggestions": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Suggestions for improvement"
        }
    },
    "required": [
        "scores", "correct_elements", "incorrect_elements",
        "missing_elements", "extra_elements", "reasoning", "suggestions"
    ]
}


class EvaluationJudge:
    """LLM-based judge for evaluating plugin outputs."""

    def __init__(self, model: str = "sonnet"):
        self.model = model

    async def evaluate(
        self,
        actual_output: str,
        expected_output: str,
        scenario_context: dict[str, Any] | None = None
    ) -> JudgeAnalysis:
        """
        Use LLM to judge the output quality.

        Args:
            actual_output: The YAML output generated by the plugin
            expected_output: The expected YAML output
            scenario_context: Optional context about the scenario

        Returns:
            JudgeAnalysis with scores and detailed analysis
        """
        if scenario_context is None:
            scenario_context = {}

        prompt = self._build_evaluation_prompt(
            actual_output,
            expected_output,
            scenario_context
        )

        options = ClaudeAgentOptions(
            system_prompt=JUDGE_SYSTEM_PROMPT,
            model=self.model,
            permission_mode="default",
            max_turns=1,
            output_format={
                "type": "json_schema",
                "schema": EVALUATION_SCHEMA
            }
        )

        response_text = ""
        structured_output = None

        async for message in query(prompt=prompt, options=options):
            # Check for structured output from ResultMessage
            if isinstance(message, ResultMessage):
                if message.structured_output:
                    structured_output = message.structured_output
            # Capture text output as fallback
            elif isinstance(message, AssistantMessage):
                for block in message.content:
                    if isinstance(block, TextBlock):
                        response_text += block.text
            elif hasattr(message, "content"):
                for block in message.content:
                    if hasattr(block, "text"):
                        response_text += block.text

        # Parse structured output or fall back to text parsing
        if structured_output:
            return self._parse_structured_response(structured_output)

        return self._parse_text_response(response_text)

    def _build_evaluation_prompt(
        self,
        actual: str,
        expected: str,
        context: dict[str, Any]
    ) -> str:
        """Build the evaluation prompt for the judge."""
        context_section = ""
        if context:
            context_section = f"""## Scenario Context
```json
{json.dumps(context, indent=2)}
```

"""

        return f"""{context_section}## Expected Output
```yaml
{expected}
```

## Actual Output
```yaml
{actual}
```

## Evaluation Task

Compare the actual output against the expected output and provide:
1. Scores for each dimension (0-100)
2. Lists of correct, incorrect, missing, and extra elements
3. Reasoning for your assessment
4. Suggestions for improvement

Focus on functional equivalence - minor formatting differences are acceptable.
Respond with JSON matching the required schema."""

    def _parse_structured_response(self, data: Any) -> JudgeAnalysis:
        """Parse structured output from SDK."""
        scores = data.get("scores", {})

        return JudgeAnalysis(
            score=EvaluationScore(
                overall=scores.get("overall", 0),
                structural=scores.get("structural", 0),
                semantic=scores.get("semantic", 0),
                completeness=scores.get("completeness", 0),
                correctness=scores.get("correctness", 0)
            ),
            correct_elements=data.get("correct_elements", []),
            incorrect_elements=data.get("incorrect_elements", []),
            missing_elements=data.get("missing_elements", []),
            extra_elements=data.get("extra_elements", []),
            reasoning=data.get("reasoning", ""),
            suggestions=data.get("suggestions", [])
        )

    def _parse_text_response(self, text: str) -> JudgeAnalysis:
        """Parse JSON from text response (fallback)."""
        # Try to find JSON in the response
        json_match = re.search(r"\{[\s\S]*\}", text)
        if json_match:
            try:
                data = json.loads(json_match.group())
                return self._parse_structured_response(data)
            except json.JSONDecodeError:
                pass

        # Return failed analysis if parsing fails
        return JudgeAnalysis(
            score=EvaluationScore(
                overall=0,
                structural=0,
                semantic=0,
                completeness=0,
                correctness=0
            ),
            correct_elements=[],
            incorrect_elements=["Failed to parse judge response"],
            missing_elements=[],
            extra_elements=[],
            reasoning=f"Could not parse evaluation response: {text[:500]}",
            suggestions=["Check judge prompt and response format"]
        )
